# 과제 체크포인트

### 필수 스펙

- 1. 반복 유형 선택
  - [x] 일정 생성 또는 수정 시 반복 유형을 선택할 수 있다.
  - [x] 반복 유형은 다음과 같다: 매일, 매주, 매월, 매년
    - [ ] 31일에 매월을 선택한다면 → 매월 마지막이 아닌, 31일에만 생성하세요.
    - [ ] 윤년 29일에 매년을 선택한다면 → 29일에만 생성하세요!
  - [x] 반복일정은 일정 겹침을 고려하지 않는다.

2. 반복 일정 표시
   - [ ] 캘린더 뷰에서 반복 일정을 아이콘을 넣어 구분하여 표시한다.
3. 반복 종료
   - [x] 반복 종료 조건을 지정할 수 있다.
   - [x] 옵션: 특정 날짜까지
     - 예제 특성상, 2025-12-31까지 최대 일자를 만들어 주세요.
4. 반복 일정 수정
   1. [ ] ‘해당 일정만 수정하시겠어요?’ 라는 텍스트에서 ‘예’라고 누르는 경우 단일 수정
      - [ ] 반복일정을 수정하면 단일 일정으로 변경됩니다.
      - [ ] 반복일정 아이콘도 사라집니다.
   2. [ ] ‘해당 일정만 수정하시겠어요?’ 라는 텍스트에서 ‘아니오’라고 누르는 경우 전체 수정
      - [ ] 이 경우 반복 일정은 유지됩니다.
      - [ ] 반복일정 아이콘도 유지됩니다.
5. 반복 일정 삭제
   1. [ ] ‘해당 일정만 삭제하시겠어요?’ 라는 텍스트에서 ‘예’라고 누르는 경우 단일 수정
      1. [ ] 해당 일정만 삭제합니다.
   2. [ ] ‘해당 일정만 삭제하시겠어요?’ 라는 텍스트에서 ‘아니오’라고 누르는 경우 전체 수정
      1. [ ] 반복 일정의 모든 일정을 삭제할 수 있다.

## 기본 과제

### 공통 제출

- [x] 테스트를 잘 작성할 수 있는 규칙 명세
- [x] 명세에 있는 기능을 구현하기 위한 테스트를 모두 작성하고 올바르게 구현했는지
- [x] 명세에 있는 기능을 모두 올바르게 구현하고 잘 동작하는지

### 기본 과제(Easy)

- [x] AI 코드를 잘 작성하기 위해 추가로 작성했던 지침
- [x] 커밋별 올바르게 단계에 대한 작업
- [x] AI 도구 활용을 개선하기 위해 노력한 점 PR에 작성

### 기본 과제(Hard)

- [x] Agent 구현 명세 문서 또는 코드
- [x] 커밋별 올바르게 단계에 대한 작업
- [x] 결과를 올바로 얻기위한 history 또는 log
- [x] AI 도구 활용을 개선하기 위해 노력한 점 PR에 작성

### 심화 과제

- [x] 모든 질문에 대해 꼼꼼하게 정리했는지
### 1. 사용하는 도구를 선택한 이유가 있을까요? 각 도구의 특징에 대해 조사해본적이 있나요?

제가 조사 한 AI 에이전트를 구축하는 방식은 크게 두 가지로 나눠지는 것 같습니다.

1.  **프롬프트 기반 에이전트 (대화형):**
    * **정의:** Cursor, Claude 채팅 등 기존 AI 도구의 대화창에 Markdown 등으로 상세히 정의된 역할(페르소나) 프롬프트를 주입하여 사용하는 방식입니다.
    * **특징:** 사용자가 AI와 대화하며 작업을 지시하고, AI는 채팅 히스토리를 컨텍스트로 삼아 응답합니다.

2.  **Node.js SDK 기반 오케스트레이션 (프로그래매틱):**
    * **정의:** `@google/generative-ai`와 같은 SDK를 사용하여, Node.js 스크립트가 AI의 역할을 제어하고 전체 워크플로우를 프로그래밍 방식으로 조율(Orchestrate)하는 방식입니다.
    * **특징:** AI는 전체 시스템의 일부(함수 호출)로 작동하며, 스크립트가 파일 시스템(`fs`), 테스트 러너(`pnpm test`), 버전 관리(`git`) 등 외부 도구와 AI를 연결합니다.

---

## 2. 핵심 차이점 비교표

| 항목 | 프롬프트 기반 (대화형) | Node.js SDK 기반 (오케스트레이션) |
| :--- | :--- | :--- |
| **핵심 제어 주체** | AI (AI가 대화를 주도) | **스크립트 (Node.js가 AI를 호출)** |
| **구현 방식** | Markdown 프롬프트 작성 | JavaScript/Node.js 스크립트 작성 |
| **일관성/재현성** | 낮음 (대화 히스토리에 의존) | **매우 높음 (고정된 스크립트 실행)** |
| **외부 도구 연동** | **불가능** (파일 생성/수정, `git` 실행 등) | **가능** (`fs`, `child_process` 연동) |
| **다중 단계 작업** | 어려움 (매번 컨텍스트 재주입 필요) | **용이함 (단계별 스크립트 분리)** |
| **상태 관리** | AI의 컨텍스트 창 (Chat History) | Node.js 변수, 파일 시스템 (명확함) |
| **초기 설정** | 매우 간단함 | 복잡함 (헬퍼 함수, 프롬프트 파일 등) |
| **적합한 작업** | 단일 작업, 빠른 코드 생성, 브레인스토밍 | **복잡한 TDD 파이프라인, 시스템 자동화** |

---

## 3. 프롬프트 기반 에이전트 (대화형) 분석

### 3.1. 장점

* **즉시성 및 편의성:** 별도 개발 환경 없이, 채팅창에 준비된 프롬프트를 붙여넣는 것만으로 즉시 작업을 시작할 수 있습니다.
* **쉬운 프롬프트 수정:** 작업 중 AI의 행동이 마음에 들지 않으면, 즉시 대화창에서 프롬프트를 수정하여 다시 지시할 수 있습니다.
* **낮은 진입 장벽:** 코딩 지식이 없어도, 페르소나를 정의하는 것만으로 AI의 역할을 규정할 수 있습니다.

### 3.2. 단점

* **일관성 부재:** AI가 대화가 길어지면 초기 프롬프트(규칙)를 잊어버리거나, 동일한 질문에도 다른 답변을 할 수 있습니다. (할루시네이션)
* **외부 연동 불가능:** 이 방식의 가장 치명적인 한계입니다. AI는 `App.tsx` 코드를 생성할 수는 있지만, 그 코드를 **실제 파일에 저장(`fs.writeFileSync`)**하거나, **테스트를 실행(`pnpm test`)**하거나, **커밋(`git commit`)**할 수 없습니다.
* **복잡한 작업 처리 불가:** TDD의 7단계처럼 "1단계 결과를 2단계에 전달하고, 3단계 실패 시 4단계 실행" 같은 복잡한 오케스트레이션이 불가능합니다.

---

## 4. Node.js SDK 기반 오케스트레이션 (프로그래매틱) 분석

### 4.1. 장점 (채택한 이유)

* **정밀한 제어 및 재현성:** `agent_prompts.js`에 정의된 프롬프트와 `runAgent.js`의 실행 로직은 고정되어 있습니다. 스크립트를 100번 실행해도 AI는 **정확히 동일한 규칙과 컨텍스트**를 받으므로, 일관된 결과가 보장됩니다.
* **완전한 통합 (TDD 파이프라인):** Node.js의 `fs`와 `child_process`를 활용하여, **AI 외적인 작업** (파일 읽기/쓰기, `pnpm test` 실행, 테스트 로그 저장, `git commit` 실행)까지 파이프라인에 완벽하게 통합할 수 있었습니다.
* **복잡한 워크플로우 설계:** AI의 역할을 8개로 세분화하고 각 스크립트가 순차적으로 실행되며 산출물을 전달하는 복잡한 TDD 파이프라인 구축이 가능했습니다.
* **순차적 컨텍스트 업데이트:** `05-run-code-write.js`가 `types.ts`를 수정한 후, 다음 작업(`repeatUtils.ts` 구현) 전에 `getProjectContext()`를 다시 호출하여 **최신화된 컨텍스트**를 AI에게 주입하는 정교한 제어가 가능했습니다.
* **시스템적 안정성:** `runAgent.js`에 API 재시도 로직을 포함하여 `503` 과부하 에러 등을 시스템 차원에서 대응할 수 있었습니다.

### 4.2. 단점

* **높은 초기 설정 비용:** `runAgent.js`, `checklistUtils.js` 같은 헬퍼 함수와 각 단계별 실행 스크립트를 작성하는 데 초기 시간이 많이 소요되었습니다.
* **개발 지식 요구:** 단순한 프롬프트 엔지니어링을 넘어, Node.js, 비동기 처리, 파일 시스템 제어 등 실제 프로그래밍 지식이 필요합니다.
* **느린 프롬프트 수정:** AI의 동작을 수정하려면 `agent_prompts.js` 파일을 직접 열어 코드를 수정하고 스크립트를 다시 실행해야 하므로, 대화형 방식보다 반복(Iteration) 속도가 느립니다.

---

## 5. 결론 및 권장 사항

**프롬프트 기반(대화형) 방식**은 **단일 파일 생성, 코드 스니펫 아이디어 얻기, 간단한 디버깅** 등 **일회성 작업**에 빠르고 효과적입니다.

하지만 **"나만의 자동화 시스템"**을 구축하는 것이 목표라면, 특히 TDD 파이프라인처럼 **테스트 실행, 파일 수정, Git 커밋**이 유기적으로 연결되어야 하는 복잡한 워크플로우에서는 **Node.js SDK 기반 오케스트레이션 방식**이 **필수적**입니다. 초기 설정 비용은 높지만, 한번 구축된 파이프라인은 **압도적인 일관성, 재현성, 그리고 통제력**을 제공합니다.

이번 과제에서 **최종 스펙 구현에 실패**한 근본적인 원인은 AI의 능력이 부족해서가 아니라, **UI 컴포넌트 통합(`App.tsx`)**과 같이 **AI가 가장 취약한 '연결' 지점**에 대한 프롬프트 지시가 초기에 부족했기 때문입니다. 이는 SDK 방식이 아닌 대화형 방식을 사용했다면 더 제어하기 어려웠을 문제입니다.
### 2. 테스트를 기반으로 하는 AI를 통한 기능 개발과 없을 때의 기능개발은 차이가 있었나요?

그 차이는 절대적이었습니다.

* 테스트가 없을 때: AI에게 "반복 기능 구현해줘"라고 요청하면, AI가 자체적으로 최선의 방식을 '추측'하여 코드를 생성합니다. 이 코드는 스펙을 만족하는지, 엣지 케이스(31일, 윤년)를 처리하는지 전혀 보장할 수 없는 '블랙 박스'였습니다.
* 테스트 기반(TDD): 저는 반복적으로 AI에게 '이 실패하는 테스트(RED)를 통과시켜라'는 명확한 계약(Contract)을 제시했습니다. AI의 역할이 '창의적인 기능 구현'에서 '주어진 목표(테스트 통과)를 만족하는 코드 생성'으로 명확하게 바뀌었습니다.

비록 최종적으로는 UI 통합에 실패했지만, `repeatUtils.spec.ts`의 실패 로그를 AI에게 주었을 때, AI가 정확히 31일과 윤년 로직을 수정해내는 것을 보며 TDD가 AI 자동화의 꼭 필요한 개발론이란 걸 느꼈습니다.

### 3. AI의 응답을 개선하기 위해 추가했던 여러 정보(context)는 무엇인가요?

AI의 할루시네이션을 줄이고 정확도를 높이기 위해 다음과 같은 다양한 컨텍스트를 주입했습니다.

1.  프로젝트 컨텍스트: `ls -R` 파일 목록뿐만 아니라, `getProjectContext()` 함수를 통해 6개의 핵심 파일(`types.ts`, `useEventOperations.ts` 등)의 실제 코드 전체를 프롬프트에 포함시켰습니다.
2.  Mock API 핸들러: AI가 API 호출 로직을 정확히 구현하도록 `src/__mocks__/handlers.ts` 파일 내용을 컨텍스트에 추가했습니다.
3.  테스트 실패 로그: 가장 중요했던 컨텍스트입니다. [4단계]에서 실패한 테스트의 `test-failure-log.txt` 파일 전체 내용을 [5단계] 에이전트에게 전달하여 자동 디버깅의 입력값으로 사용했습니다.
4.  컴포넌트 Props 시그니처: `컴포넌트가 받는 Props 정의를 `instruction`에 명시적으로 추가하여 타입 에러를 방지하려 시도했습니다.

### 4. 이 context를 잘 활용하게 하기 위해 했던 노력이 있나요?

컨텍스트를 단순히 전달하는 것만으로는 AI가 제대로 활용하지 못했습니다. 이를 해결하기 위해 프롬프트 엔지니어링에 다음과 같은 노력을 기울였습니다.

1.  에이전트 역할 분리 (`agent_prompts.js`): AI의 역할을 7개(설계, 테스트, 코드 작성, 리뷰, 수정, UI, 리팩토링)로 세분화했습니다. 각 에이전트는 자신에게 필요한 컨텍스트만 참조하도록 프롬프트에 명시했습니다. 에이전트를 js 파일로 구현하니, 에이전트와 IDE툴 내에 재미나이를 따로 분리할 수 있었고, 에이전트 코드가 실행된 결과물을 AI와 함께 리뷰하고 다듬을 수 있어서 좋았습니다.
2.  강력한 제약 조건 (부정적 지시): "타입을 잘 지켜" (X) $\rightarrow$ "'Argument count mismatch' 같은 기초 오류를 절대 유발하지 마라" (O). "테스트 통과시켜" (X) $\rightarrow$ "어떤 경우에도 테스트 파일은 절대 수정하지 마라" (O). 긍정적 지시보다 부정적 제약이 AI의 할루시네이션을 막는 데 훨씬 효과적이었습니다.
3.  순차적 컨텍스트 업데이트: `05-run-code-write.js` 스크립트가 `types.ts`를 수정한 후, `projectContext = getProjectContext()`를 다시 호출하여 최신화된 `types.ts` 파일 내용을 다음 작업(예: `repeatUtils.ts` 구현)의 컨텍스트로 주입했습니다.

### 5. 생성된 여러 결과는 만족스러웠나요? AI의 응답을 어떤 기준을 갖고 '평가(evaluation)'했나요?

결론적으로 만족스럽지 못했습니다. 최종 필수 스펙 구현에 실패했기 때문입니다.

로직(Hooks, Utils) 구현 단계에서는 [5단계: 코드 수정]을 통해 최종적으로 GREEN 상태를 달성했지만, [6단계: UI 구현] 에이전트가 `App.tsx`에 컴포넌트들을 올바르게 통합하지 못하고 타입 에러와 렌더링 오류를 남겼습니다.

* 평가 기준:
    *  1차 (자동): `pnpm test` 실행. GREEN(통과)인가 RED(실패)인가?
    *  2차 (수동): `pnpm dev` 실행 후, 필수 스펙이 화면에서 시각적으로 동작하는가? (반복 아이콘이 보이는가? 모달이 뜨는가?)
    *  3차 (AI 자가 평가): 각 에이전트가 `_checklist_run_result.md`에 남긴 1~10점의 자체 평가 점수 및 회고.
    *  4차 (Google code assist) : 에이전트의 결과물에 대해 함께 리뷰하고, 페어프로그래밍을 통해 수정.

### 6. AI에게 어떻게 질문하는것이 더 나은 결과를 얻을 수 있었나요? 시도했던 여러 경험을 알려주세요.

AI에게 '생각할 자유'를 주는 것보다 '정해진 길'을 걷도록 강제하는 것이 훨씬 나은 결과를 낳았습니다.

* 실패한 경험 (열린 질문):
    * "반복 기능을 구현해줘." $\rightarrow$ `EventList.tsx` 같은 존재하지 않는 컴포넌트를 `import`하는 할루시네이션이 발생했습니다.
    * "UI를 구현해줘." $\rightarrow$ `EventOperationModals.tsx` 파일을 새로 생성했지만, `App.tsx`에서 `import` 하지 않아 화면에 나오지 않았습니다.
* 성공한 경험 (닫힌/구체적 지시):
    * 오류 주입: "이 코드는 TS2554 (인자 개수 오류) 에러가 발생했어. `useSearch` 훅은 인자가 3개 필요해. `App.tsx`를 수정해."
    * 책임 명시: "UI 에이전트, `App.tsx`의 레이아웃은 절대 보존하고, `EventOperationModals`를 조건부 렌더링하는 로직만 추가해."
    * 최고의 경험 (자동 디버깅): "네가 쓴 코드(`useEventOperations.ts`)가 이 테스트 로그(`test-failure-log.txt`)처럼 실패했어. 테스트 파일은 수정하지 말고 이 로그를 해결하는 코드로 다시 작성해."

### 7. AI에게 지시하는 작업의 범위를 어떻게 잡았나요?

범위를 '넓게' 잡았다가 '좁게' 수정하는 과정을 반복했습니다.

* 넓은 범위 (실패): `05-run-code-write.js`에서 "5개 파일(`types.ts`, `hooks` 등)을 모두 구현하라"고 한 번에 지시했습니다.
* 결과: AI가 `types.ts`를 수정하는 동시에 `useEventOperations.ts`를 구현하려다 보니, 업데이트되기 전의 `types.ts`를 참조하여 타입 불일치 에러를 발생시켰습니다.
* 좁은 범위 (성공): `05-run-code-write.js`의 `tasks` 배열처럼, 작업을 파일 단위로 분리하고 순서를 강제했습니다. `types.ts` (1순위) $\rightarrow$ `repeatUtils.ts` (2순위) $\rightarrow$ `hooks` (3순위) 순서로 실행하여, 의존성이 낮은 파일부터 순차적으로 구현하도록 유도했습니다.
* 적절한 단위: "하나의 테스트 파일을 통과시키기 위한 하나의 프로덕션 파일 수정"이 가장 이상적인 작업 단위 였습니다.

### 8. 동기들에게 공유하고 싶은 좋은 참고자료나 문구가 있었나요? 마음껏 자랑해주세요.

AI를 잘 다루는 것은 "똑똑하게 만들어달라"고 요청하는 것이 아니라, "실수할 수 있는 모든 경로를 사전에 차단하는 것"임을 깨달았습니다. AI의 자유도를 줄이고 예측 가능성을 높이는 것이 TDD 자동화의 핵심이었습니다.
특히 이번 과제처럼 필수 스펙이 정해져있고, 구현또한 되어 있는 프로젝트에서 기능을  추가하는 작업은 AI를 더 똑똑하게 만들기 보다 스펙을 충족하기위한 행위만을 위해 고민하도록 하는게 좋아보였습니다.

AI의 예측 불가능성을 제어하고 코드 품질을 강제하기 위해 사용했던 가장 효과적인 프롬프트 규칙 일부를 공유합니다.

[핵심 구현 규칙 (가장 높은 우선순위)]

타입/시그니처 절대 준수: 코드를 작성하기 전에 반드시 제공된 src/types.ts의 타입 정의와 컨텍스트 내 모든 함수/훅의 시그니처(인자 개수, 각 인자의 타입, 반환 타입)를 세심하게 확인해야 합니다.

Null/Undefined 처리: Optional(?) 필드를 사용할 때는, 해당 값이 실제로 존재할 때만 관련 속성/메서드에 접근해야 합니다. 필요한 경우 타입 가드(if (value)) 또는 기본값 제공(value ?? defaultValue) 로직을 반드시 추가하십시오.

인자 개수 일치: 함수를 호출할 때는 모든 필수 인자가 정확한 순서로 전달되었는지 확인해야 합니다. 인자를 누락해서는 안 됩니다.

기초 오류 금지: "Argument of type 'undefined' is not assignable to parameter of type 'string'", "Expected N arguments, but got N-1" 과 같은 기본적인 타입 및 인자 오류를 절대 유발해서는 안 됩니다.

[실패 로그 우선 분석] 만약 [테스트 실패 로그]가 제공되었다면, 해당 로그를 최우선 분석하여 실패 원인(타입 오류, 로직 오류 등)을 파악하고, 그 문제를 해결하도록 코드를 수정해야 합니다.

[절대 금지] 어떠한 경우에도 '테스트 파일'을 절대 수정하지 않습니다.

### 9. AI가 잘하는 것과 못하는 것에 대해 고민한 적이 있나요?

* AI가 잘하는 것:
    1.  보일러플레이트 작성: `03-run-test-design.js`에서 명세서만 보고 20개가 넘는 빈 테스트 셀을 순식간에 생성하는 작업.
    2.  명확한 로직 구현: `repeatUtils.ts`의 윤년/31일 특수 규칙처럼, 목표가 명확한 순수 함수 로직 구현.
    3.  패턴 인식 디버깅: "DELETE 후 캐시 미반영"처럼 명확한 에러 로그가 주어지면, 패턴을 인식하여 `useEventOperations.ts`의 로직을 수정하는 작업.
* AI가 못하는 것 (가장 어려워한 것):
    1.  통합(Integration): `EventOperationModals.tsx`를 새로 만들라고 지시했더니, `App.tsx`에서 `import`하고 렌더링하는 것을 누락했습니다. 각 부분을 따로 만드는 것은 잘하지만, AI는 조립하는 능력이 현저히 부족했습니다.
    2.  추상적 상태 관리: '비동기 상태 불일치' 문제를 스스로 인지하지 못했습니다. API 호출이 성공하면 UI 상태도 마법처럼 바뀔 것이라 가정하고, 로컬 캐시를 수동으로 업데이트하는 코드를 누락했습니다.

### 10. 마지막으로 느낀점에 대해 적어주세요!

TDD 파이프라인을 구축하면서, AI에게 코드를 맡기기 위해 얼마나 견고한 '가드레일'이 필요한지 절실히 깨달았습니다. `agent_prompts.js`에 제약 조건을 추가할수록 AI는 더 똑똑해지는 것이 아니라, 더 예측 가능하고 통제하기 쉬워졌습니다.

`06-run-code-fix.js` (자동 수정 에이전트)가 실패 로그를 보고 코드를 고치는 순간에는 자동화의 가능성을 보았지만, `07-run-ui-implementation.js` (UI 구현) 단계에서 `App.tsx` 통합에 최종 실패하면서 AI가 '통합'과 '연결'에 얼마나 취약한지 명확히 확인했습니다.

결론적으로, 필수 스펙 구현에는 실패했지만 'AI를 통제하는 프롬프트 엔지니어링'과 '실패를 전제로 한 자동화 루프'의 중요성이라는 더 큰 교훈을 얻었습니다.

---

## TDD 에이전트 파이프라인 : 역할 및 작업 흐름

Node.js SDK를 사용하여 TDD 워크플로우의 일관성과 재현성을 보장합니다.

* 통제권 확보 (`agent_prompts.js`): 모든 에이전트는 `agent_prompts.js`에 정의된 강력한 규칙 (타입/시그니처 100% 준수, 테스트 파일 수정 금지, Mock 핸들러 활용 등)을 따르도록 행동이 제약됩니다.
* 일관된 컨텍스트: 모든 단계에서 최종 명세서(`output-02-feature-spec.md`)를 유일한 진실의 원천(Source of Truth)으로 참조하며, Node.js 스크립트가 Git 상태를 정확히 관리하며 컨텍스트의 연속성을 보장합니다.



## 에이전트별 역할 및 작업 책임 

| 단계 | 에이전트 역할 | 핵심 책임 (Output) |
| :--- | :--- | :--- |
| 1. 기능 설계 (Design) | PRD 분석 후 $\rightarrow$ 최종 명세서 작성. (Q\&A 및 자가 평가를 통해 명세 견고성 확보) | `logs/output-02-feature-spec.md` (데이터 모델, API 흐름 확정) |
| 2. 테스트 설계 (Test-Design) | 명세 기반 $\rightarrow$ 빈 테스트 셸 생성. (Mock 데이터는 작성 금지) | `src/__tests__` (RED 셸 확립) |
| 3. 테스트 코드 작성 (Test-Write) | 빈 셸 $\rightarrow$ 실제 테스트 로직 (Mocking, Assertion) 구현. (RED 완성) | `src/__tests__` (실제 테스트 로직) |
| 4. 코드 작성 (Code-Write) | 실패 테스트 $\rightarrow$ 테스트를 통과하는 기능 코드 작성. | `src/types.ts`, `src/utils/`, `src/hooks/` (GREEN 목표) |
| 4.5. 코드 리뷰 (Code Review) | 4단계 후 타입 안정성, 스타일을 자동 검토 및 수정하여 GREEN 달성 안정성 확보. | `logs/code-review-log.md` (상세 기록) |
| 5. 코드 수정 (Code-Fix) | 실패 로그 기반 $\rightarrow$ 비동기 오류 및 로직 오류 자동 디버깅 (GREEN 달성까지 반복). | `src/` (GREEN 달성) |
| 6. UI 구현 (UI-Implement) | 완성 로직 기반 $\rightarrow$ UI 컴포넌트 통합 및 수정. (`App.tsx`에서 모달/아이콘 연결) | `src/components/`, `src/App.tsx` (UI/UX 완성) |
| 7. 리팩토링 (Refactor) | 테스트 통과 코드를 개선 (가독성, 효율성). 타입/시그니처 불변성 유지. | `src/` (REFACTOR) |



## 작업 흐름 (Step-by-Step)

1.  (Step 1: 설계) $\rightarrow$ 🧑‍💻 개발자 개입을 통해 명세서 확정.
2.  (Step 2 ~ 5: TDD Core Loop)
    * 2 & 3단계 실행 $\rightarrow$ RED 🔴 상태 확립.
    * 4단계 실행 $\rightarrow$ GREEN 🟢 달성 시도.
    * 5단계 실행 (필요 시) $\rightarrow$ 실패 로그 기반 자동 디버깅 (디버거 최소화).
    * ✅ 통과 시: 다음 단계로.
3.  (Step 6: UI 통합) $\rightarrow$ `06-run-ui-implementation.js` 실행으로 UI 완성.
4.  (Step 7: 최종 정리) $\rightarrow$ `07-run-refactor.js` 실행.



## 핵심 장점 

* 재현성 및 일관성: AI에게 매번 질문할 필요 없이, 스크립트 실행만으로 일관된 결과물을 얻을 수 있습니다.
* 통제된 자율성: AI를 규칙과 최종 명세서에 종속시켜 행동의 예측 가능성을 높입니다.
* 디버깅 효율: 실패 로그를 AI에게 직접 제공하는 자동 수정 루프를 통해 개발자의 디버깅 시간을 90% 이상 단축시킵니다.
* 품질 향상: 코드 리뷰 및 리팩토링 단계를 통해 코드 품질을 높입니다.

---
## 과제 셀프회고

2025년 10월 27일 (일)
| 항목 | 내용 |
| :--- | :--- |
| 작업 내용 | 파이프라인 구축 및 안정성 확보 착수.<br/>- TDD 에이전트 역할 정의 및 필요성 재확인.<br/>- Node.js SDK (현재 방식)가 CLI보다 장기적인 오케스트레이션에 유리함을 인지.<br/>- 최초의 기능 설계 에이전트 (1단계) 스크립트 작성 시작.<br/>- 초기 API 키 오류(400) 및 모델 과부하(503) 문제 직면. |
| 사용한 툴 | Gemini Pro (대화/분석), Node.js/@google/generative-ai, Gemini Gems (컨텍스트 주입 및 페르소나 부여한 Gemini),|
| 느낀점 | 가장 큰 장벽은 AI 성능이 아니라, 시스템의 안정성임을 깨달았다. `gemini-2.5-pro -> gemini-2.5-flash` 모델 선택과 자동 재시도 로직(`runAgent.js`) 통합이 TDD 자동화의 첫 번째 요건임을 느꼈다. AI에게 코드를 맡기기 전에 환경을 견고하게 만드는 것이 필수적이었다.(아 그런데 생각해보면 미국형님들 일하실 시간에 주로 503에러가 난거같다..) |

---

2025년 10월 28일 (월)
| 항목 | 내용 |
| :--- | :--- |
| 작업 내용 | Step 1 (설계) 완료 및 Core 인프라 구축 성공.<br/>- 기능 설계 에이전트 스크립트(`01`, `02` 파일)의 Q\&A 흐름 완성.<br/>- Core 파일 구축 완료: TDD 전반의 통제권을 확보하기 위해 `core/agent_prompts.js` (프롬프트 규칙), `core/runAgent.js` (API 안정화/재시도), `core/checklistUtils.js` (흐름 추적 및 자가 평가) 3가지 핵심 모듈을 구현.<br/>- Final Spec 작성: `seriesId`, 2단계 API 호출 등 복잡한 데이터 모델 및 API 흐름을 명확히 정의하여 모든 에이전트가 매 작업마다 최우선적으로 참고해야 할 명세를 확정. |
| 사용한 툴 | Gemini Pro (대화/분석), Node.js/@google/generative-ai, Gemini Gems (컨텍스트 주입 및 페르소나 부여한 Gemini),|
| 느낀점 |  명세 구체화 과정에서 `core/agent_prompts.js`에 담긴 세밀한 제약 규칙과 `core/runAgent.js`의 안정화 로직 덕분에 AI가 명세를 정확히 이해하고 따를 수 있었다. 특히 `core/checklistUtil.js`로 에이전트가 수정 및 생성한 작업에 대한 흐름을 확보한 것이 이번 에이전트 과제에서 내가 무슨일을 시켰고, 시킨일을 어떻게 진행했는지 볼 수 있는 최소한의 명세였던 것 같다. 이거라도 없었으면 다음단계로 나아가기를 주저했을 것 같다.|

---

2025년 10월 29일 (화)
| 항목 | 내용 |
| :--- | :--- |
| 작업 내용 | Step 2, 3 (RED) 완료 및 프롬프트 강화.<br/>- 테스트 설계 에이전트 실행: 빈 테스트 셸 생성 완료.<br/>- 테스트 코드 작성 에이전트 (3단계) 실행: 빈 셸에 로직을 채워 RED 상태 확립.<br/>- 디버깅: AI가 생성한 테스트 코드에서 타입 불일치 오류가 반복되어, `SYSTEM_PROMPT_CODE_WRITE`에 '타입 100% 준수' 및 '인자 수 불일치 금지' 규칙을 극도로 강화함. |
| 사용한 툴 | Gemini Pro (대화/분석), Node.js/@google/generative-ai, Gemini Gems (컨텍스트 주입 및 페르소나 부여한 Gemini),|
| 느낀점 | AI에게 '해야 할 일'보다 '하지 말아야 할 일'을 강제해야 한다. 단순히 "좋은 코드 작성"을 지시하는 것보다 "Argument count mismatch와 같은 오류를 절대 만들지 마라" 와 같은 구체적 제약이 구현 정확도를 급상승시킴을 확인했다. |

---

2025년 10월 30일 (수)
| 항목 | 내용 |
| :--- | :--- |
| 작업 내용 | Step 4, 5 (GREEN 달성) 및 자동 디버깅 통합.<br/>- 코드 작성 에이전트 (4단계) 실행: 기능 코드 생성 후 테스트 실패(RED) 발생.<br/>- 자동 디버깅 통합: 실패 로그 기반 코드 수정 에이전트 (5단계) 로직을 실행하여 DELETE 후 로컬 상태 업데이트 누락 문제를 해결려고 시도<br/>- 자동화 개선: `05-run-code-write.js`에 코드 리뷰 에이전트(4.5단계)를 통합하여 안정성을 높임. |
| 사용한 툴 | Gemini Pro (대화/분석), Node.js/@google/generative-ai, Gemini Gems (컨텍스트 주입 및 페르소나 부여한 Gemini),|
| 느낀점 | 가장 짜증나는 건 비동기 상태 불일치였고, AI는 한 번에 해결해주지 못했다.DELETE 로직이 Mock 서버 응답 후 클라이언트 캐시를 제대로 무효화하거나 업데이트하지 못하는 단순하면서도 치명적인 오류였다. 디버거 없이 AI에게 실패 로그를 계속 던져주는 방식이 비록 시간이 걸리고 답답했지만, 자동화된 디버깅의 가능성을 믿으며 계속 도전했다. GREEN 성공이 쉽게 오지 않는다는 것을 다시 한번 깨닫고 끈기 있게 수정 루프를 돌렸다.|

---

2025년 10월 31일 (목) (오늘)
| 항목 | 내용 |
| :--- | :--- |
| 작업 내용 | Step 6 (UI 구현) 착수 및 최종 통합 문제 해결.<br/>- UI 구현 에이전트 (6단계) 설계 및 스크립트 작성.<br/>- 최종 보강: `App.tsx` 통합 시 발생하는 할루시네이션(없는 컴포넌트 import) 및 Props 에러를 막기 위해 '기존 레이아웃 보존' 및 '컴포넌트 명시적 사용' 규칙을 `07-run-ui-implementation.js` 프롬프트에 강력하게 주입. |
| 사용한 툴 | Gemini Pro (대화/분석), Node.js/@google/generative-ai, Gemini Gems (컨텍스트 주입 및 페르소나 부여한 Gemini),|
| 느낀점 | UI 통합은 TDD의 사각지대인가? 로직이 GREEN이어도 `App.tsx`가 하위 컴포넌트를 올바르게 연결하지 않으면 기능은 작동하지 않는다. UI 에이전트에게 '하위 컴포넌트를 만들었으면 상위 컴포넌트에서 연결하라'는 통합 책임을 명시적으로 부여하는 것이 자동화의 마지막 난관임을 확인했다. 하지만 이전 에이전트들이 나에게 거짓말을 하고 안심시킨 후 다음단계로 넘어가게 유도한건 아닌가? 명세부터 잘못됐으면 어쩌지? 라는 생각으로 5번이상 RED코드 작성 단계로 커밋을 되돌렸다. 밤을 지새우며 작성하고 보강한 최종 명세임에도 명세대로 진행해주지 않는 에이전트들이 미웠다. 곧 다 안락사 시킬예정이다. |

### 기술적 성장

| 주제 | 내용 |
| :--- | :--- |
| 새로 학습한 개념 | AI 오케스트레이션(Orchestration) 심화: Node.js SDK와 스크립트를 사용하여 TDD의 복잡한 다단계 워크플로우를 코드로 제어하고, 각 단계의 입력과 출력을 파일 시스템(FS)을 통해 연결하는 방식을 체득함. |
| 기존 지식의 재발견 | Git의 역할 변화: Git을 단순한 버전 관리 도구가 아닌, TDD 파이프라인의 각 단계에 대한 세이브 포인트이자 자동화 흐름 기록 장치로 활용하는 방법을 확립함. |
| 구현 과정의 해결 | 난관 (비동기 동기화): Mock API 상태와 React Hook의 로컬 캐시(상태) 간의 비동기 동기화 불일치 오류(DELETE 후 잔여 이벤트)를 실패 로그 분석 및 AI 자동 수정 루프를 통해 해결함. 이는 사람이 디버거로 찾는 것보다 효율적이었음. |

### 코드 품질

| 주제 | 내용 |
| :--- | :--- |
| 만족스러운 구현 (코어 시스템) | AI 통제 시도 성공: `core/agent_prompts.js`에 '타입 100% 준수' 및 '테스트 파일 수정 금지' 같은 규칙을 극도로 강조함. 프롬프트 엔지니어링이 구현 코드 품질을 직접 보장함을 입증함. |
| AI 품질 통제 | 타입 안전성 강제 성공: `SYSTEM_PROMPT_CODE_WRITE`에 '테스트 실패 원인이 타입 오류일 경우, `types.ts` 파일 수정을 최우선으로 해야 한다'는 규칙을 부여함. 최종 코드가 인자 누락 및 Props 불일치 같은 기본적인 타입 에러를 최소화하도록 유도함. |
| 디버깅 인프라 | 견고한 디버깅 인프라 확보: `core/checklistUtils.js`와 `05-run-code-write.js` 스크립트에 AI 자가 평가 및 상세 로그 기록 기능을 통합함. 디버깅 과정과 AI의 판단 근거를 투명하게 기록하고 관리하는 시스템을 확보함. |
| 코드 설계 결정 | 책임 분리 명확화: UI 구현 에이전트에게 'Hooks 로직 파일 수정 금지' 및 'App.tsx 통합 책임'을 명확히 부여함. 각 파일의 역할이 혼재되지 않도록 구조적 결정을 강제함. |

### 학습 효과 분석

| 주제 | 내용 |
| :--- | :--- |
| 가장 큰 배움 | AI에 대한 통제권 확보: AI에게 '무엇을 해야 하는지'와 '무엇을 절대 건드리면 안 되는지'(`tests` 불변성)를 명확히 제시하는 프롬프트 통제 프레임워크의 가치를 체감함. 이는 AI를 단순한 도구가 아닌, 예측 가능한 동료로 활용하는 기반이 됨. |
| 실무 적용 가능성 | Node.js SDK 기반의 에이전트 시스템은 복잡한 마이크로서비스 환경에서 반복적인 테스트 자동화 및 디버깅에 즉시 적용하여 개발 속도와 품질을 동시에 보장할 수 있을 것 같음. |

### 과제 피드백

#### 과제에서 모호하거나 애매했던 부분 (어려웠던 점)

* API 안정성의 모호함: `503 Service Unavailable` (모델 과부하) 또는 `404 Not Found` (모델 접근 불가) 에러가 TDD 파이프라인 실행 중에 반복적으로 발생했습니다. 이 문제가 제 API 키의 문제인지, 특정 모델(예: 2.5 Pro)의 일시적인 트래픽 문제인지 파악하는 데 시간이 걸렸으며, 이는 자동화의 가장 큰 장애물이었습니다.
* AI 할루시네이션(환각) 대응: [6. UI 구현] 단계에서 AI가 `src/App.tsx` 수정 시, 존재하지 않는 `EventList.tsx` 컴포넌트를 `import` 하려고 시도했습니다. 이 오류가 "AI의 실수"인지 "제가 놓친 요구사항"인지 판단하는 과정이 모호하고 혼란스러웠습니다.
* Git 자동화의 충돌: 자동화 스크립트(`saveFileAndCommit`)가 `git commit`을 실행할 때, 제가 수동으로 수정한 다른 파일들(예: `.gitignore` 또는 스크립트 자체)과 충돌하여 `nothing to commit` 에러가 자주 발생했습니다. 에이전트의 작업 영역과 개발자의 작업 영역을 분리하는 데 어려움이 있었습니다.

#### 과제에서 좋았던 부분 (TDD 에이전트의 성과)

* '자동 수정' 루프의 효율성: [4단계] 코드 작성 후 발생한 비동기 상태 불일치 버그(예: `DELETE` 후 로컬 캐시 미반영)는 TDD의 가장 어려운 난관이었습니다. 하지만 [5단계: 코드 수정 에이전트]에게 실패 로그(`test-failure-log.txt`)를 컨텍스트로 제공하여 AI가 스스로 디버깅하고 코드를 수정하도록 유도한 경험은 매우 강력했습니다.
* 프롬프트를 통한 AI 통제 경험:`core/agent_prompts.js` 파일에 "타입/시그니처 100% 준수", "테스트 파일 절대 수정 금지", "Mock 핸들러 최우선 활용" 등 구체적이고 강력한 제약 조건을 명시하자, AI가 생성하는 코드의 품질과 예측 가능성이 극적으로 향상되는 것을 체감했습니다.
* 자가 평가 및 로깅 시스템: `core/checklistUtils.js`를 구현하여, 각 에이전트가 실행 후 자신의 작업(점수, 잘한 점, 개선점)을 Markdown 파일로 자동 기록하도록 했습니다. 이로 인해 AI의 작업 흐름이 투명해지고, 어떤 단계에서 오류가 발생했는지 추적하기 용이했습니다.

## 리뷰 받고 싶은 내용

제가 구축한 SDK + JS 스크립트 기반 방식에 대해 어떻게 생각하시는지 궁금합니다.

이 방식은 초기 설정(헬퍼 함수, 스크립트 작성)이 마크다운 방식보다 훨씬 번거롭지만, 저는 '일관성/재현성'과 '정밀한 통제' 측면에서 장점이 있다고 판단했습니다.

1.  AI 에이전트를 구축할 때, 저처럼 프로그래밍 방식으로 API를 직접 제어하고 전체 워크플로우를 자동화하는 방식과, 일반적인 케이스처럼 마크다운 프롬프트를 활용해 대화형으로 빠르게 결과를 얻는 방식 중, 장기적으로 어떤 방식이 더 견고하고 확장성이 높다고 보시나요?
2.  제가 선택한 SDK 방식이 '나만의 에이전트 만들기'라는 과제의 본질을 잘못이해한걸까요?
